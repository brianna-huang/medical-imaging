{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Class imbalance fixes\n",
        "\n",
        "We will once again be returning to our pneumonia data set with a special emphasis on class imbalances. The goal will be to explore various methods to overcome these imbalances with the larger aim of increasing our precision/sensitivity."
      ],
      "metadata": {
        "id": "weWxZWrW0M_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You can greatly improve the computation speed in Google Colab by connecting to a GPU. Click the \"Runtime\" tab in the top ribbon, then \"Change runtime type\". You can then select \"T4 GPU\". Note, however, that GPUs are subject to availability; Google has a fixed (and unspecified) amount of resources available at any given time, so a GPU may not be available. Feel free to try again later if you don't succeed at first."
      ],
      "metadata": {
        "id": "jVdt4lCaViio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries to Import"
      ],
      "metadata": {
        "id": "KC06p_hI2oYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.18.0\n",
        "!pip install keras==3.8.0\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import pickle"
      ],
      "metadata": {
        "id": "pnV7_ulw2eRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a53c42-a232-48cc-a1e0-6ef65fc8a11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
            "Requirement already satisfied: keras==3.8.0 in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras==3.8.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras==3.8.0) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.8.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.8.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.8.0) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real quick**: make sure tensorflow and keras are version 2.18.0 and 3.8.0, respectively by running the cells below.\n",
        "\n",
        "If either is showing the wrong version, restart the session by clicking the \"Runtime\" tab up top and selecting \"Restart session\". After that, run the notebook again from the top."
      ],
      "metadata": {
        "id": "kfrcbh_ltkYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow #Should be version 2.18.0"
      ],
      "metadata": {
        "id": "-P7ymezBtmml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747e26af-7286-470c-ab06-688bee13f412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.18.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tensorflow-text, tensorflow_decision_forests, tf_keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show keras #Should be version 3.8.0"
      ],
      "metadata": {
        "id": "cCcJ7zsItqiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699c30f6-39f6-4085-8d13-e344d0904264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: keras\n",
            "Version: 3.8.0\n",
            "Summary: Multi-backend Keras\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Keras team <keras-users@googlegroups.com>\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
            "Required-by: tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seed():\n",
        "  seed = 18\n",
        "  # Set random seeds for reproducibility\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "  # (For TensorFlow GPU determinism)\n",
        "  os.environ['CUDA_VISBLE_DEVICE'] = ''\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "  os.environ['PYTHONHASHSEED'] = str(1234)"
      ],
      "metadata": {
        "id": "-m7tFXdr3I_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Loading the Image Data"
      ],
      "metadata": {
        "id": "HJBiI0BR0NHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in HW4, upload the imbalanced X-ray image files as follows. :\n",
        "\n",
        "\n",
        "1.   Upload the files `imbalanced_xray_train.pkl`, `imbalanced_xray_test.pkl`, and `imbalanced_xray_val.pkl` from your computer into Colab's file tree. You can either drag and drop these files from your computer or use the upload button in Colab. Be sure you are uploading the **imbalanced** data!\n",
        "2.  Run the cells below to populate the train, test, and val variables.\n"
      ],
      "metadata": {
        "id": "AzzkQHWB0NLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell, but DO NOT EDIT\n",
        "def get_data(pkl_path):\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "      # Read the data from the file\n",
        "      data = pickle.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "NlvKdtcWl3to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell, but DO NOT EDIT\n",
        "\n",
        "train = get_data(\"imbalanced_xray_train.pkl\")\n",
        "test = get_data(\"imbalanced_xray_test.pkl\")\n",
        "val = get_data(\"imbalanced_xray_val.pkl\")"
      ],
      "metadata": {
        "id": "HPCwY5WsXLne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the name imbalanced_xray implies, the key difference between the dataset used in this homework and that used in the prior assigment is the number of examples of each data class. In the imbalanced dataset, there are roughly 4 times as many training instances of Pneumonia lungs as there are Normal lungs. The goal of this homework is to explore the effect of this imbalance as well as ways to overcome it."
      ],
      "metadata": {
        "id": "ETrC2t7w0NWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Preparation"
      ],
      "metadata": {
        "id": "0TjkZKZ20NYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Separate the image data from their corresponding labels into x_train, x_val, x_test, y_train, y_val, and y_test arrays.\n",
        "2.   Normalize the x data by dividing by 255.\n",
        "3.   Reshape x_train, x_val, and x_test such that they are numpy arrays of shape (|x|, 150, 150, 1)\n",
        "4.   Convert y_train, y_val, and y_test to numpy arrays.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vylry96bZ6xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [train[i][0] for i in range(len(train))]\n",
        "y_train = [train[i][1] for i in range(len(train))]\n",
        "\n",
        "x_val = [val[i][0] for i in range(len(val))]\n",
        "y_val = [val[i][1] for i in range(len(val))]\n",
        "\n",
        "x_test = [test[i][0] for i in range(len(test))]\n",
        "y_test = [test[i][1] for i in range(len(test))]\n",
        "\n",
        "# normalize\n",
        "x_train = np.array(x_train) / 255\n",
        "x_val = np.array(x_val) / 255\n",
        "x_test = np.array(x_test) / 255\n",
        "\n",
        "# reshape\n",
        "x_train = x_train.reshape(x_train.shape[0], 150, 150, 1)\n",
        "x_val = x_val.reshape(x_val.shape[0], 150, 150, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 150, 150, 1)\n",
        "\n",
        "# convert labels to numpy\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "CYpNWDM6Y-wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3a: Naive Random Oversampling"
      ],
      "metadata": {
        "id": "1_rm9KaMbN8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we train any CNNs, we want to try to address the class imbalance. One way we can do so is with oversampling methods, the most naive of which is to generate new samples by randomly sampling with replacement the available training data.\n",
        "\n",
        "In the cell below, we use imblearn's RandomOverSampler with a random state of 0 to create resampled `x_train` and `y_train` datasets for use in a CNN."
      ],
      "metadata": {
        "id": "0XWzO9WdbuEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42\n",
        "\n",
        "x_resampled, y_resampled = RandomOverSampler(random_state=random_state).fit_resample(x_train.reshape((x_train.shape[0], 150*150)), y_train)"
      ],
      "metadata": {
        "id": "Jk8MrcA4bXZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we need to reshape our `x` data one more time for use in our CNN. Reshape `x_resampled` such that it is an array of shape (-1, 150, 150, 1)."
      ],
      "metadata": {
        "id": "LUY45Tc68ln6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_resampled = x_resampled.reshape(-1, 150, 150, 1)"
      ],
      "metadata": {
        "id": "kSV2dgyQAN89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(y_train))\n",
        "print(Counter(y_resampled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57IiVzYYIhs1",
        "outputId": "c98188d5-8732-444c-e8ea-9784cdbae8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({np.int64(1): 3875, np.int64(0): 970})\n",
            "Counter({np.int64(0): 3875, np.int64(1): 3875})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3b: Random Oversampled CNN"
      ],
      "metadata": {
        "id": "dlXnhrMoAmF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now train a CNN on both the standard data and the resampled data. For the first run, train the network below using the unmodified `x_train` and `y_train`. Run the evaluation cells below to see the class imbalance's effect on the model's recall. Next, train the model again using `x_resampled` and `y_resampled`. Once again, generate evaluation metrics to observe the effect that oversampling had on the model's recall."
      ],
      "metadata": {
        "id": "jTU_zrDlBc9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_random_seed()\n",
        "\n",
        "model_oversample = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_oversample.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "num_epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "# model_oversample.fit(x = x_train, y = y_train, epochs=num_epochs, validation_data = (x_val, y_val), batch_size=batch_size)\n",
        "model_oversample.fit(x = x_resampled, y = y_resampled, epochs=num_epochs, validation_data = (x_val, y_val), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "7Jmqn6VZAkWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c80c65-ddd5-4556-bee1-327b2685cc30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - binary_accuracy: 0.8233 - loss: 0.3442 - precision_23: 0.8446 - recall_23: 0.7931 - val_binary_accuracy: 0.8750 - val_loss: 0.2091 - val_precision_23: 0.8000 - val_recall_23: 1.0000\n",
            "Epoch 2/2\n",
            "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - binary_accuracy: 0.9706 - loss: 0.0835 - precision_23: 0.9744 - recall_23: 0.9665 - val_binary_accuracy: 0.9375 - val_loss: 0.1752 - val_precision_23: 0.8889 - val_recall_23: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec1b7a39950>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cells below to generate evaluation metrics for your model. Pay particular attention to the recall metric for the Normal (0) class."
      ],
      "metadata": {
        "id": "kOUd3Poh6gdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluations for model trained on unmodified x_train and y_train\n",
        "predictions = model_oversample.predict(x_test)\n",
        "binary_predictions = np.where(predictions > 0.5, 1, 0)\n",
        "print(classification_report(y_test, binary_predictions, target_names = ['Normal (Class 0)','Pneumonia (Class 1)']))"
      ],
      "metadata": {
        "id": "1V2pezs3K50c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92609ec-f78c-4154-d248-1bf8ceb38e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "   Normal (Class 0)       0.94      0.41      0.57       234\n",
            "Pneumonia (Class 1)       0.74      0.98      0.84       390\n",
            "\n",
            "           accuracy                           0.77       624\n",
            "          macro avg       0.84      0.70      0.71       624\n",
            "       weighted avg       0.81      0.77      0.74       624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_oversample.evaluate(x_test,y_test)\n",
        "print(f\"Accuracy: {results[1]}\")\n",
        "print(f\"Precision: {results[2]}\")\n",
        "print(f\"Recall: {results[3]}\")"
      ],
      "metadata": {
        "id": "jjpiOgiQ-BVg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ab661e-d689-4e48-b884-7341bed848c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.6035 - loss: 4.7761 - precision_2: 0.3738 - recall_2: 0.6596\n",
            "Accuracy: 0.7692307829856873\n",
            "Precision: 0.7356321811676025\n",
            "Recall: 0.9846153855323792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "There are very few true normal samples and a large number of false negatives\n",
        "(labeled pneumonic but actually normal) since there are many more pneumonia samples.\n",
        "so it makes sense that recall for the normal class is low, and recall for the\n",
        "pneumonia class is high.\n",
        "\n",
        "The normal sample (class 0) recall value decreases when we use the resampled dataset.\n",
        "It's possible that the model overfit to the duplicated normal images, but still failed\n",
        "to generalize to new ones.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ms1uszRoFRCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluations for model trained on resampled data\n",
        "predictions = model_oversample.predict(x_test)\n",
        "binary_predictions = np.where(predictions > 0.5, 1, 0)\n",
        "print(classification_report(y_test, binary_predictions, target_names = ['Normal (Class 0)','Pneumonia (Class 1)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pNsShT2Ep26",
        "outputId": "9e4d70fc-6333-4697-c9bf-85a3e15ca8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "   Normal (Class 0)       0.98      0.42      0.59       234\n",
            "Pneumonia (Class 1)       0.74      0.99      0.85       390\n",
            "\n",
            "           accuracy                           0.78       624\n",
            "          macro avg       0.86      0.71      0.72       624\n",
            "       weighted avg       0.83      0.78      0.75       624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_oversample.evaluate(x_test,y_test)\n",
        "print(f\"Accuracy: {results[1]}\")\n",
        "print(f\"Precision: {results[2]}\")\n",
        "print(f\"Recall: {results[3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2r7CVTYEz7i",
        "outputId": "fdfbf787-6c09-4f3f-a125-dd319b9c2c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - binary_accuracy: 0.6094 - loss: 1.8689 - precision_23: 0.3777 - recall_23: 0.6633\n",
            "Accuracy: 0.7804487347602844\n",
            "Precision: 0.7418738007545471\n",
            "Recall: 0.9948717951774597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you're ready, save the model trained on `x_resampled` and `y_resampled`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6sMhJz2S_DU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to mount your Google Drive (click through pop-up windows to authenticate)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UDXfoCZOgE8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98843ba-d5ca-45ff-dbee-12eeb8e7b181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/EAS5860\" #(Ex: \"drive/MyDrive/EAS5860/HW5\")\n",
        "\n",
        "model_oversample.save(os.path.join(path, \"EAS5860_HW5_Part_3.keras\"))"
      ],
      "metadata": {
        "id": "UEOL2Z5F_VWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4a: Random Undersampling"
      ],
      "metadata": {
        "id": "PPdvo3VuLJys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to oversampling the minority class, we can undersample the majority class. In the cell below, implement imblearn's RandomUnderSampler to create undersampled versions of x_train and y_train. You can read more about the under sampling method here: [click me](https://imbalanced-learn.org/stable/under_sampling.html)\n",
        "\n",
        "As before, be mindful of reshaping your data."
      ],
      "metadata": {
        "id": "xQ98jKySBAES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state= 42\n",
        "\n",
        "x_undersampled, y_undersampled = RandomUnderSampler(random_state=random_state).fit_resample(x_train.reshape((x_train.shape[0], 150*150)), y_train)"
      ],
      "metadata": {
        "id": "rRNrCpZUM-2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_undersampled = x_undersampled.reshape(-1, 150, 150, 1)"
      ],
      "metadata": {
        "id": "at_ooCIJC6Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4b: Random Undersampled CNN"
      ],
      "metadata": {
        "id": "McMnZ554DWe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the network below using `x_undersampled` and `y_undersampled`."
      ],
      "metadata": {
        "id": "v_yftZM3EAea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_random_seed()\n",
        "\n",
        "model_undersample = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_undersample.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "model_undersample.fit(x = x_undersampled, y = y_undersampled, epochs=num_epochs, validation_data = (x_val, y_val), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "DqHAW3W1D_RI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925ecbe2-80b3-4196-9ced-b086be921b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - binary_accuracy: 0.7807 - loss: 0.4724 - precision_19: 0.8344 - recall_19: 0.7109 - val_binary_accuracy: 0.7500 - val_loss: 0.6303 - val_precision_19: 0.6667 - val_recall_19: 1.0000\n",
            "Epoch 2/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - binary_accuracy: 0.9524 - loss: 0.1406 - precision_19: 0.9541 - recall_19: 0.9518 - val_binary_accuracy: 0.6250 - val_loss: 0.9435 - val_precision_19: 0.5714 - val_recall_19: 1.0000\n",
            "Epoch 3/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - binary_accuracy: 0.9522 - loss: 0.1239 - precision_19: 0.9532 - recall_19: 0.9533 - val_binary_accuracy: 1.0000 - val_loss: 0.1167 - val_precision_19: 1.0000 - val_recall_19: 1.0000\n",
            "Epoch 4/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - binary_accuracy: 0.9661 - loss: 0.0971 - precision_19: 0.9721 - recall_19: 0.9616 - val_binary_accuracy: 1.0000 - val_loss: 0.1069 - val_precision_19: 1.0000 - val_recall_19: 1.0000\n",
            "Epoch 5/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - binary_accuracy: 0.9675 - loss: 0.0835 - precision_19: 0.9720 - recall_19: 0.9647 - val_binary_accuracy: 1.0000 - val_loss: 0.0969 - val_precision_19: 1.0000 - val_recall_19: 1.0000\n",
            "Epoch 6/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - binary_accuracy: 0.9671 - loss: 0.0720 - precision_19: 0.9747 - recall_19: 0.9617 - val_binary_accuracy: 1.0000 - val_loss: 0.0732 - val_precision_19: 1.0000 - val_recall_19: 1.0000\n",
            "Epoch 7/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - binary_accuracy: 0.9797 - loss: 0.0575 - precision_19: 0.9818 - recall_19: 0.9794 - val_binary_accuracy: 0.9375 - val_loss: 0.1354 - val_precision_19: 1.0000 - val_recall_19: 0.8750\n",
            "Epoch 8/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - binary_accuracy: 0.9727 - loss: 0.0564 - precision_19: 0.9784 - recall_19: 0.9686 - val_binary_accuracy: 0.9375 - val_loss: 0.1180 - val_precision_19: 0.8889 - val_recall_19: 1.0000\n",
            "Epoch 9/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - binary_accuracy: 0.9931 - loss: 0.0193 - precision_19: 0.9944 - recall_19: 0.9919 - val_binary_accuracy: 0.9375 - val_loss: 0.0579 - val_precision_19: 1.0000 - val_recall_19: 0.8750\n",
            "Epoch 10/10\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - binary_accuracy: 0.9903 - loss: 0.0234 - precision_19: 0.9949 - recall_19: 0.9867 - val_binary_accuracy: 0.9375 - val_loss: 0.0935 - val_precision_19: 0.8889 - val_recall_19: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec1b9532690>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, run the cells below to generate evaluation metrics for your undersampled model. Pay particular attention to the recall metric for the Normal (0) class.\n",
        "\n",
        "How does this compare to the oversampled model? How about the basic model?"
      ],
      "metadata": {
        "id": "5eAecv_TE-1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_undersample.predict(x_test)\n",
        "binary_predictions = np.where(predictions > 0.5, 1, 0)\n",
        "print(classification_report(y_test, binary_predictions, target_names = ['Normal (Class 0)','Pneumonia (Class 1)']))"
      ],
      "metadata": {
        "id": "QvoWM_JUBRx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e739b4-6deb-437a-f8ed-c5ae5f382a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "   Normal (Class 0)       0.91      0.53      0.67       234\n",
            "Pneumonia (Class 1)       0.77      0.97      0.86       390\n",
            "\n",
            "           accuracy                           0.80       624\n",
            "          macro avg       0.84      0.75      0.76       624\n",
            "       weighted avg       0.82      0.80      0.79       624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_undersample.evaluate(x_test,y_test)\n",
        "print(f\"Accuracy: {results[1]}\")\n",
        "print(f\"Precision: {results[2]}\")\n",
        "print(f\"Recall: {results[3]}\")"
      ],
      "metadata": {
        "id": "L6K1lW-1BZqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cd1788-e36a-490e-fc2b-85bc263588e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - binary_accuracy: 0.6734 - loss: 1.8212 - precision_19: 0.4031 - recall_19: 0.6532\n",
            "Accuracy: 0.8028846383094788\n",
            "Precision: 0.7730061411857605\n",
            "Recall: 0.9692307710647583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The undersampled model performs better than both the oversampled and basic model.\n",
        "This makes sense, because there is no class imbalance, nor the issue of overfitting\n",
        "due to duplicated samples.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5K4LndfwM4jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you're ready, save the model trained on `x_undersampled` and `y_undersampled`"
      ],
      "metadata": {
        "id": "seNLlJeCBsuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/EAS5860\"\n",
        "\n",
        "model_undersample.save(os.path.join(path, \"EAS5860_HW5_Part_4.keras\"))"
      ],
      "metadata": {
        "id": "49xyjZBNBmX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5a: Synthetic Minority Oversampling Technique"
      ],
      "metadata": {
        "id": "MiO84-fTNEyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prior oversampling method simply reused training images multiple times. The Synthetic Minority Oversampling Technique (SMOTE), by comparison, creates new synthetic images that are similar to, but distinct from the existing training data. You can read more about how SMOTE works here: [click me](https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c)\n",
        "\n",
        "In the cell below, use imblearn's SMOTE to generate `x_smote` and `y_smote` datasets from the original `x_train` and `y_train` (reference imblearn documentation for details). As before, be mindful to reshape your data."
      ],
      "metadata": {
        "id": "OxcSsOxkCuj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42\n",
        "\n",
        "x_smote, y_smote = SMOTE(random_state=random_state).fit_resample(x_train.reshape((x_train.shape[0], 150*150)), y_train)"
      ],
      "metadata": {
        "id": "XSRNj7svR1o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_smote = x_smote.reshape(-1, 150, 150, 1)"
      ],
      "metadata": {
        "id": "M1FiWd9gGIhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5b: SMOTE CNN"
      ],
      "metadata": {
        "id": "bSYdKUipICQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the network below using `x_smote` and `y_smote`. You will need to specify the number of epochs and batch size to do so."
      ],
      "metadata": {
        "id": "62F9EDM5g8zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_random_seed()\n",
        "\n",
        "model_smote = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_smote.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "num_epochs = 7\n",
        "batch_size = 16\n",
        "\n",
        "model_smote.fit(x = x_smote, y = y_smote, epochs=num_epochs, validation_data = (x_val, y_val), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zQyX6I9HHgze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4e9218-1915-425f-938e-9e0d4081f775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.8522 - loss: 0.3114 - precision_21: 0.8680 - recall_21: 0.8343 - val_binary_accuracy: 0.8125 - val_loss: 0.4788 - val_precision_21: 1.0000 - val_recall_21: 0.6250\n",
            "Epoch 2/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9723 - loss: 0.0882 - precision_21: 0.9780 - recall_21: 0.9668 - val_binary_accuracy: 1.0000 - val_loss: 0.0857 - val_precision_21: 1.0000 - val_recall_21: 1.0000\n",
            "Epoch 3/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9812 - loss: 0.0609 - precision_21: 0.9857 - recall_21: 0.9768 - val_binary_accuracy: 0.9375 - val_loss: 0.2218 - val_precision_21: 0.8889 - val_recall_21: 1.0000\n",
            "Epoch 4/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - binary_accuracy: 0.9860 - loss: 0.0390 - precision_21: 0.9884 - recall_21: 0.9835 - val_binary_accuracy: 1.0000 - val_loss: 0.0459 - val_precision_21: 1.0000 - val_recall_21: 1.0000\n",
            "Epoch 5/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - binary_accuracy: 0.9910 - loss: 0.0269 - precision_21: 0.9935 - recall_21: 0.9885 - val_binary_accuracy: 1.0000 - val_loss: 0.0061 - val_precision_21: 1.0000 - val_recall_21: 1.0000\n",
            "Epoch 6/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - binary_accuracy: 0.9915 - loss: 0.0224 - precision_21: 0.9931 - recall_21: 0.9900 - val_binary_accuracy: 0.9375 - val_loss: 0.1300 - val_precision_21: 0.8889 - val_recall_21: 1.0000\n",
            "Epoch 7/7\n",
            "\u001b[1m485/485\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9954 - loss: 0.0169 - precision_21: 0.9964 - recall_21: 0.9943 - val_binary_accuracy: 1.0000 - val_loss: 0.0551 - val_precision_21: 1.0000 - val_recall_21: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec1b8c20c90>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, run the cells below to generate evaluation metrics for your SMOTE model. Pay particular attention to the recall metric for the Normal (0) class.\n",
        "\n",
        "How does this compare to the previous models?"
      ],
      "metadata": {
        "id": "uLHbptrrhSBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_smote.predict(x_test)\n",
        "binary_predictions = np.where(predictions > 0.5, 1, 0)\n",
        "print(classification_report(y_test, binary_predictions, target_names = ['Normal (Class 0)','Pneumonia (Class 1)']))"
      ],
      "metadata": {
        "id": "uF3bONgYIOeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4258be82-a026-48ea-e20d-9122a4e3363d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "   Normal (Class 0)       0.94      0.46      0.61       234\n",
            "Pneumonia (Class 1)       0.75      0.98      0.85       390\n",
            "\n",
            "           accuracy                           0.79       624\n",
            "          macro avg       0.84      0.72      0.73       624\n",
            "       weighted avg       0.82      0.79      0.76       624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_smote.evaluate(x_test,y_test)\n",
        "print(f\"Accuracy: {results[1]}\")\n",
        "print(f\"Precision: {results[2]}\")\n",
        "print(f\"Recall: {results[3]}\")"
      ],
      "metadata": {
        "id": "pkfbRMrtiO13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59bf59a8-b5ec-47a2-d877-ba72d7558825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - binary_accuracy: 0.5868 - loss: 4.8470 - precision_11: 0.3682 - recall_11: 0.6576\n",
            "Accuracy: 0.7612179517745972\n",
            "Precision: 0.7286527752876282\n",
            "Recall: 0.9846153855323792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This model performs pretty well, but a bit worse than undersampling. Like oversampling,\n",
        "SMOTE creates additional samples of the minority class. But it creates synthetic\n",
        "samples, ones that don't actually exist, and could be unrealistic for what real samples\n",
        "look like. We've avoided the overfitting of oversampling, but the recall remains pretty\n",
        "low for the normal class since we don't have a variety of real samples that reflect\n",
        "what new samples look like.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "s6jkFli4Pw82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you're ready, save the model trained on `x_smote` and `y_smote`"
      ],
      "metadata": {
        "id": "ntRjbDvQicIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/EAS5860/\"\n",
        "\n",
        "model_smote.save(os.path.join(path, \"EAS5860_HW5_Part_5.keras\"))"
      ],
      "metadata": {
        "id": "AuwuHvb1iYro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Binary Focal Crossentropy"
      ],
      "metadata": {
        "id": "1bdARI_1Rq6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One final method we will look at to address class imbalances is the Binary Focal Crossentropy loss function.\n",
        "\n",
        "In Keras, `BinaryFocalCrossentropy` introduces a focusing mechanism that downweights the contribution of examples that are easier to classify (these often come from the majority class) and focuses more on the challenging minority class examples.\n",
        "\n",
        "It achieves this by introducing two hyperparameters:\n",
        "\n",
        "1.   **gamma**: \tA focusing parameter used to compute the focal factor, default is 2.0 as mentioned in the reference [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n",
        "2.   **alpha**: A weight balancing factor for class 1, default is 0.25 as mentioned in reference [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf). The weight for class 0 is 1.0 - alpha.\n",
        "\n",
        "In the cell below, specify a BinaryFocalCrossentropy loss function to be used when the model is compiled. Experiment with different alpha and gamma values and observe the overall effect on the model's evaluation metrics. Feel free to adjust other parameters in the loss function as well."
      ],
      "metadata": {
        "id": "Jrqc_0e6kFUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_random_seed()\n",
        "\n",
        "model_bfce = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "loss_function = tf.keras.losses.BinaryFocalCrossentropy(gamma=3.0, alpha=0.2)\n",
        "\n",
        "model_bfce.compile(optimizer='adam', loss= loss_function, metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "num_epochs = 15\n",
        "batch_size = 32\n",
        "\n",
        "model_bfce.fit(x = x_train, y = y_train, epochs=num_epochs, validation_data = (x_val, y_val), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Mz2VsbstNOiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82be41fb-7caf-477c-d958-4eff6274e4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - binary_accuracy: 0.8345 - loss: 0.0683 - precision_13: 0.8655 - recall_13: 0.9348 - val_binary_accuracy: 0.6250 - val_loss: 0.1228 - val_precision_13: 0.5714 - val_recall_13: 1.0000\n",
            "Epoch 2/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - binary_accuracy: 0.9499 - loss: 0.0204 - precision_13: 0.9699 - recall_13: 0.9661 - val_binary_accuracy: 0.8125 - val_loss: 0.0601 - val_precision_13: 0.7273 - val_recall_13: 1.0000\n",
            "Epoch 3/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - binary_accuracy: 0.9691 - loss: 0.0113 - precision_13: 0.9800 - recall_13: 0.9806 - val_binary_accuracy: 0.8750 - val_loss: 0.0280 - val_precision_13: 0.8000 - val_recall_13: 1.0000\n",
            "Epoch 4/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - binary_accuracy: 0.9719 - loss: 0.0094 - precision_13: 0.9817 - recall_13: 0.9826 - val_binary_accuracy: 0.9375 - val_loss: 0.0165 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n",
            "Epoch 5/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - binary_accuracy: 0.9780 - loss: 0.0070 - precision_13: 0.9875 - recall_13: 0.9845 - val_binary_accuracy: 0.9375 - val_loss: 0.0210 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n",
            "Epoch 6/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - binary_accuracy: 0.9842 - loss: 0.0056 - precision_13: 0.9911 - recall_13: 0.9888 - val_binary_accuracy: 0.9375 - val_loss: 0.0134 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n",
            "Epoch 7/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - binary_accuracy: 0.9911 - loss: 0.0034 - precision_13: 0.9961 - recall_13: 0.9927 - val_binary_accuracy: 0.8750 - val_loss: 0.0334 - val_precision_13: 0.8000 - val_recall_13: 1.0000\n",
            "Epoch 8/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - binary_accuracy: 0.9897 - loss: 0.0028 - precision_13: 0.9937 - recall_13: 0.9932 - val_binary_accuracy: 0.9375 - val_loss: 0.0278 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n",
            "Epoch 9/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - binary_accuracy: 0.9833 - loss: 0.0089 - precision_13: 0.9910 - recall_13: 0.9877 - val_binary_accuracy: 1.0000 - val_loss: 0.0085 - val_precision_13: 1.0000 - val_recall_13: 1.0000\n",
            "Epoch 10/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - binary_accuracy: 0.9948 - loss: 0.0018 - precision_13: 0.9969 - recall_13: 0.9964 - val_binary_accuracy: 1.0000 - val_loss: 0.0030 - val_precision_13: 1.0000 - val_recall_13: 1.0000\n",
            "Epoch 11/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - binary_accuracy: 0.9946 - loss: 0.0024 - precision_13: 0.9970 - recall_13: 0.9961 - val_binary_accuracy: 0.8750 - val_loss: 0.0538 - val_precision_13: 0.8000 - val_recall_13: 1.0000\n",
            "Epoch 12/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - binary_accuracy: 0.9940 - loss: 0.0024 - precision_13: 0.9971 - recall_13: 0.9952 - val_binary_accuracy: 0.8750 - val_loss: 0.0470 - val_precision_13: 0.8000 - val_recall_13: 1.0000\n",
            "Epoch 13/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - binary_accuracy: 0.9951 - loss: 0.0018 - precision_13: 0.9976 - recall_13: 0.9962 - val_binary_accuracy: 0.9375 - val_loss: 0.0866 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n",
            "Epoch 14/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - binary_accuracy: 0.9969 - loss: 0.0012 - precision_13: 0.9986 - recall_13: 0.9974 - val_binary_accuracy: 0.9375 - val_loss: 0.0397 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n",
            "Epoch 15/15\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - binary_accuracy: 0.9983 - loss: 7.4624e-04 - precision_13: 0.9988 - recall_13: 0.9991 - val_binary_accuracy: 0.9375 - val_loss: 0.0227 - val_precision_13: 0.8889 - val_recall_13: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec1bb2d22d0>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, run the cells below to generate evaluation metrics for your BinaryFocalCrossEntropy model. Pay particular attention to the recall metric for the Normal (0) class. Models with a recall metric in the Normal (0) class >= 0.35 will receive full credit.\n",
        "\n",
        "How does this compare to the previous models?"
      ],
      "metadata": {
        "id": "G-dP4ihlsoh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model_bfce.predict(x_test)\n",
        "binary_predictions = np.where(predictions > 0.5, 1, 0)\n",
        "print(classification_report(y_test, binary_predictions, target_names = ['Normal (Class 0)','Pneumonia (Class 1)']))"
      ],
      "metadata": {
        "id": "LTe5Jlsss69g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40b3324-b28d-4fbc-eb34-8e2949037a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "   Normal (Class 0)       0.97      0.41      0.57       234\n",
            "Pneumonia (Class 1)       0.74      0.99      0.84       390\n",
            "\n",
            "           accuracy                           0.77       624\n",
            "          macro avg       0.85      0.70      0.71       624\n",
            "       weighted avg       0.82      0.77      0.74       624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_bfce.evaluate(x_test,y_test)\n",
        "print(f\"Accuracy: {results[1]}\")\n",
        "print(f\"Precision: {results[2]}\")\n",
        "print(f\"Recall: {results[3]}\")"
      ],
      "metadata": {
        "id": "qDWiwx8Rs9Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01419917-685e-406c-a357-cad3570d6442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - binary_accuracy: 0.5927 - loss: 1.1175 - precision_13: 0.3732 - recall_13: 0.6618\n",
            "Accuracy: 0.7724359035491943\n",
            "Precision: 0.7357414364814758\n",
            "Recall: 0.9923076629638672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This model performs pretty well too, but a bit worse than undersampling. By using\n",
        "BinaryFocalCrossentropy, we can tailor the loss function to downweight easy examples\n",
        "(class 1: pneumonia) and put higher weight onto the more difficult ones for our model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RGkqRQm_SsNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you're ready, save the BinaryFocalCrossEntropy model"
      ],
      "metadata": {
        "id": "YJ20W7ybtSyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/EAS5860/\"\n",
        "\n",
        "model_bfce.save(os.path.join(path, \"EAS5860_HW5_Part_6.keras\"))"
      ],
      "metadata": {
        "id": "yETYgD3DtYso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}